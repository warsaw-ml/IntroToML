{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrootutils\n",
    "root = pyrootutils.setup_root(\".\", pythonpath=True, cwd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import rich\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.face_age_dataset import FaceAgeDataset\n",
    "from src.data.face_age_dataset_from_path import FaceAgeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    img_size = (224, 224)\n",
    "    max_age = 80\n",
    "    val_size = 1600\n",
    "    test_size = 1600\n",
    "    max_imgs_per_class = 500\n",
    "    oversample_with_augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "transform = [transforms.Normalize(imagenet_mean, imagenet_std)]\n",
    "\n",
    "ds = FaceAgeDataset(\n",
    "    data_dir=\"data/\",\n",
    "    img_size=cfg.img_size,\n",
    "    label_clipping=(1, cfg.max_age),\n",
    "    normalize_labels=False,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([1]), 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, idx = ds[0]\n",
    "x.shape, y.shape, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders for new dataset\n",
    "os.makedirs(\"data/face_age_dataset/train\", exist_ok=True)\n",
    "os.makedirs(\"data/face_age_dataset/val\", exist_ok=True)\n",
    "os.makedirs(\"data/face_age_dataset/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_per_val_class = int(cfg.val_size / cfg.max_age)\n",
    "imgs_per_test_class = int(cfg.test_size / cfg.max_age)\n",
    "\n",
    "used_idxs = set() # img ids used for validation and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23708/23708 [00:39<00:00, 594.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val imgs: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_counts_val = Counter()\n",
    "\n",
    "for x, y, idx in tqdm(DataLoader(ds, shuffle=True)):\n",
    "    idx = int(idx)\n",
    "    y = int(y)\n",
    "    count = class_counts_val[y]\n",
    "    \n",
    "    if count < imgs_per_val_class and idx not in used_idxs:\n",
    "        class_counts_val[y] += 1\n",
    "        used_idxs.add(idx)\n",
    "        x = x.squeeze(0)\n",
    "        torch.save(x, f\"data/face_age_dataset/val/{idx}_age_{y}.pt\")\n",
    "        \n",
    "print(\"Val imgs:\", sum(class_counts_val.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23708/23708 [00:34<00:00, 691.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test imgs: 1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_counts_test = Counter()\n",
    "\n",
    "for x, y, idx in tqdm(DataLoader(ds, shuffle=True)):\n",
    "    idx = int(idx)\n",
    "    y = int(y)\n",
    "    count = class_counts_test[y]\n",
    "    \n",
    "    if count < imgs_per_test_class and idx not in used_idxs:\n",
    "        class_counts_test[y] += 1\n",
    "        used_idxs.add(idx)\n",
    "        x = x.squeeze(0)\n",
    "        torch.save(x, f\"data/face_age_dataset/test/{idx}_age_{y}.pt\")\n",
    "        \n",
    "print(\"Test imgs:\", sum(class_counts_test.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23708/23708 [01:41<00:00, 234.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train imgs: 16535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_counts_train = Counter()\n",
    "\n",
    "for x, y, idx in tqdm(DataLoader(ds, shuffle=True)):\n",
    "    idx = int(idx)\n",
    "    y = int(y)\n",
    "    count = class_counts_train[y]\n",
    "    \n",
    "    if count < cfg.max_imgs_per_class and idx not in used_idxs:\n",
    "        class_counts_train[y] += 1\n",
    "        x = x.squeeze(0)\n",
    "        torch.save(x, f\"data/face_age_dataset/train/{idx}_age_{y}.pt\")\n",
    "\n",
    "print(\"Train imgs:\", sum(class_counts_train.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before augmentation:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [05:53<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After augmentation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Before augmentation:\")\n",
    "plt.bar(class_counts_train.keys(), class_counts_train.values())\n",
    "plt.show()\n",
    "\n",
    "transform_list = [\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5, fill=0),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "]\n",
    "transform = transforms.Compose(transform_list)\n",
    "\n",
    "if cfg.oversample_with_augmentation:\n",
    "    os.makedirs(\"data/face_age_dataset/train_augmented\", exist_ok=True)\n",
    "    augmented_imgs_count = 0\n",
    "    \n",
    "    # iterate over labels\n",
    "    for label in tqdm(range(1, cfg.max_age + 1)):\n",
    "        \n",
    "        # load all images for label\n",
    "        img_paths = [x for x in os.listdir(\"data/face_age_dataset/train\") if f\"age_{label}.pt\" in x]\n",
    "        imgs = [torch.load(f\"data/face_age_dataset/train/{x}\") for x in img_paths]\n",
    "        \n",
    "        if not imgs:\n",
    "            continue\n",
    "        \n",
    "        # augment images while there are less than max_imgs_per_class\n",
    "        augmented_imgs = []\n",
    "        while len(imgs) + len(augmented_imgs) < cfg.max_imgs_per_class:\n",
    "            for img in imgs:\n",
    "                \n",
    "                if len(imgs) + len(augmented_imgs) >= cfg.max_imgs_per_class:\n",
    "                    break\n",
    "                \n",
    "                augmented_imgs_count += 1\n",
    "                class_counts_train[label] += 1\n",
    "                augmented_imgs.append(transform(img))\n",
    "                \n",
    "        for idx, img in enumerate(augmented_imgs):\n",
    "            torch.save(img, f\"data/face_age_dataset/train_augmented/augmented_{idx}_age_{label}.pt\")\n",
    "        \n",
    "print(\"After augmentation:\")\n",
    "plt.bar(class_counts_train.keys(), class_counts_train.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counter({70: 500, 34: 500, 52: 500, 3: 500, 22: 500, 31: 500, 29: 500, 35: 500, 37: 500, 21: 500, 59: 500, 6: 500, 4: 500, 2: 500, 26: 500, 17: 500, 72: 500, 24: 500, 1: 500, 32: 500, 12: 500, 28: 500, 7: 500, 43: 500, 5: 500, 27: 500, 38: 500, 30: 500, 25: 500, 45: 500, 49: 500, 42: 500, 23: 500, 18: 500, 16: 500, 39: 500, 8: 500, 19: 500, 53: 500, 15: 500, 36: 500, 10: 500, 9: 500, 50: 500, 40: 500, 58: 500, 61: 500, 54: 500, 65: 500, 80: 500, 46: 500, 51: 500, 47: 500, 60: 500, 55: 500, 63: 500, 20: 500, 14: 500, 44: 500, 56: 500, 75: 500, 78: 500, 66: 500, 41: 500, 48: 500, 62: 500, 73: 500, 33: 500, 57: 500, 76: 500, 68: 500, 67: 500, 13: 500, 64: 500, 11: 500, 69: 500})\n",
      "Val Counter({60: 20, 42: 20, 35: 20, 22: 20, 23: 20, 26: 20, 28: 20, 1: 20, 17: 20, 6: 20, 80: 20, 10: 20, 30: 20, 24: 20, 36: 20, 18: 20, 37: 20, 41: 20, 14: 20, 53: 20, 54: 20, 3: 20, 46: 20, 20: 20, 38: 20, 58: 20, 31: 20, 67: 20, 72: 20, 2: 20, 34: 20, 9: 20, 8: 20, 63: 20, 73: 20, 4: 20, 32: 20, 55: 20, 52: 20, 29: 20, 27: 20, 69: 20, 50: 20, 66: 20, 21: 20, 47: 20, 79: 20, 51: 20, 62: 20, 40: 20, 39: 20, 16: 20, 12: 20, 44: 20, 45: 20, 61: 20, 65: 20, 25: 20, 33: 20, 75: 20, 13: 20, 48: 20, 19: 20, 56: 20, 7: 20, 15: 20, 49: 20, 78: 20, 68: 20, 70: 20, 5: 20, 59: 20, 57: 20, 76: 20, 43: 20, 11: 20, 64: 20, 71: 20, 77: 20, 74: 20})\n",
      "Test Counter({2: 20, 26: 20, 56: 20, 29: 20, 75: 20, 21: 20, 23: 20, 40: 20, 18: 20, 28: 20, 31: 20, 25: 20, 65: 20, 80: 20, 38: 20, 42: 20, 41: 20, 39: 20, 10: 20, 3: 20, 27: 20, 70: 20, 50: 20, 60: 20, 35: 20, 7: 20, 1: 20, 8: 20, 58: 20, 4: 20, 15: 20, 36: 20, 30: 20, 34: 20, 20: 20, 47: 20, 16: 20, 54: 20, 68: 20, 45: 20, 57: 20, 24: 20, 13: 20, 5: 20, 53: 20, 32: 20, 33: 20, 63: 20, 6: 20, 55: 20, 14: 20, 48: 20, 67: 20, 11: 20, 46: 20, 12: 20, 43: 20, 37: 20, 49: 20, 61: 20, 73: 20, 52: 20, 62: 20, 22: 20, 9: 20, 66: 20, 59: 20, 19: 20, 17: 20, 64: 20, 51: 20, 69: 20, 76: 20, 44: 20, 72: 20, 78: 20, 71: 13, 74: 12, 77: 8, 79: 3})\n",
      "76\n",
      "80\n",
      "80\n",
      "Train imgs: 38000\n",
      "Val imgs: 1600\n",
      "Test imgs: 1556\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\", class_counts_train)\n",
    "print(\"Val\", class_counts_val)\n",
    "print(\"Test\", class_counts_test)\n",
    "\n",
    "print(len(class_counts_train.keys()))\n",
    "print(len(class_counts_val.keys()))\n",
    "print(len(class_counts_test.keys()))\n",
    "\n",
    "print(\"Train imgs:\", sum(class_counts_train.values()))\n",
    "print(\"Val imgs:\", sum(class_counts_val.values()))\n",
    "print(\"Test imgs:\", sum(class_counts_test.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8bb4dd7ff00fca7c8e4254171e99909a303c03842f517d85c0effa7a3dfad5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
